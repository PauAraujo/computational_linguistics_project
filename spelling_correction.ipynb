{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30240aac-64d2-4430-963b-efaf37780c9f",
   "metadata": {},
   "source": [
    "# Spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "075ec16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import spacy\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a274bd",
   "metadata": {},
   "source": [
    "## Run a statistical language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c603a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class LM(object):\n",
    "    \n",
    "    def __init__(self, corpus, ngram_size=2, bos='+', eos='#', k=1):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param corpus (list): a list of sentences, where each sentence is a list of tokens. \n",
    "        :param ngram_size (integer): specifies the size of the n-grams used for language modeling (default=2). \n",
    "        :param k (float or integer): smoothing value used when computing n-gram probabilities (default=1).\n",
    "        :param bos (string): the Beginning-of-Sentence symbol (default='+').\n",
    "        :param eos (string): the End-of-Sentence symbol (default='#').\n",
    "        \n",
    "        This class creates an LM object with attributes: \n",
    "            - k (float or integer): smoothing value. \n",
    "            - ngram_size (integer): size of the n-grams. \n",
    "            - bos (string): beginning-of-sentence symbol.  \n",
    "            - eos (string): end-of-sentence symbol. \n",
    "            - corpus (nested list): a list of lists of strings representing the corpus.\n",
    "            - vocab (set): tokens derived from the corpus. \n",
    "            - vocab_size (integer): size of the vocabulary. \n",
    "            - counts (default dict): stores n-gram counts. \n",
    "        \"\"\"\n",
    "        \n",
    "        self.k = k\n",
    "        self.ngram_size = ngram_size\n",
    "        self.bos = bos\n",
    "        self.eos = eos\n",
    "        self.corpus = corpus\n",
    "        self.vocab = self.get_vocab()\n",
    "        self.vocab.add(self.eos)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "    def get_vocab(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        :return: a set of unique tokens in the corpus.\n",
    "        \n",
    "        This method derives the vocabulary from the training corpus (stored as a set of unique tokens). \n",
    "        \"\"\"\n",
    "        \n",
    "        vocab = set()\n",
    "        for sentence in self.corpus:\n",
    "            for element in sentence:\n",
    "                vocab.add(element)\n",
    "        \n",
    "        return vocab\n",
    "                    \n",
    "    def update_counts(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        :return: None\n",
    "        \n",
    "        This method creates a defaultdict, padds the sentences with EoS and BoS, and updates the n-gram counts for the LM object. \n",
    "        \"\"\"\n",
    "        \n",
    "        r = self.ngram_size - 1\n",
    "        \n",
    "        self.counts = defaultdict(dict)\n",
    "        \n",
    "        for sentence in self.corpus:\n",
    "            s = [self.bos]*r + sentence + [self.eos]\n",
    "            \n",
    "            for idx in range(self.ngram_size-1, len(s)):\n",
    "                ngram = self.get_ngram(s, idx)\n",
    "                \n",
    "                try:\n",
    "                    self.counts[ngram[0]][ngram[1]] += 1\n",
    "                except KeyError:\n",
    "                    self.counts[ngram[0]][ngram[1]] = 1\n",
    "                        \n",
    "    def get_ngram(self, s, i):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param s (list): tokens representing the sentence being processed.\n",
    "        :param i (integer): index of the target token in the sentence.\n",
    "        :return (tuple): the history and the target for the n-gram.\n",
    "        \n",
    "        This method extracts the n-gram for the current position (i) in the list of tokens (s),\n",
    "        the n-gram consists of n-1 history tokens and 1 target token. \n",
    "        \"\"\"\n",
    "        \n",
    "        ngram = s[i-(self.ngram_size-1):i+1]\n",
    "        history = tuple(ngram[:-1])\n",
    "        target = ngram[-1]\n",
    "        return (history, target)\n",
    "    \n",
    "    def get_ngram_probability(self, history, target):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param history (tuple): n-1 tokens representing the history of the n-gram.\n",
    "        :param target (string): the target token of the n-gram, for which the probability is computed \n",
    "        :return (float): probability of the target given the observed history\n",
    "        \n",
    "        This method computes the probability of a target token given the history in the LM object.\n",
    "        This probability is based on the smoothed counts of n-grams found in the corpus. \n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            ngram_tot = np.sum(list(self.counts[history].values())) + (self.vocab_size*self.k)\n",
    "            try:\n",
    "                transition_count = self.counts[history][target] + self.k\n",
    "            except KeyError:\n",
    "                transition_count = self.k\n",
    "        except KeyError:\n",
    "            transition_count = self.k\n",
    "            ngram_tot = self.vocab_size*self.k\n",
    "        \n",
    "        return transition_count/ngram_tot \n",
    "    \n",
    "    def perplexity(self, sentence):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param sentence (list): tokens representing the sentence.\n",
    "        :return (float): perplexity score of the sentence.\n",
    "        \n",
    "        This method computes the perplexity score of a given sentence using the formula pow(2.0, avg_entropy).\n",
    "        This tells us how well the language model predicts the unseen sentence.\n",
    "        \"\"\"\n",
    "        \n",
    "        r = self.ngram_size - 1\n",
    "        s = [self.bos]*r + sentence + [self.eos]\n",
    "        \n",
    "\n",
    "        probs = []\n",
    "        for idx in range(self.ngram_size-1, len(s)):\n",
    "            ngram = self.get_ngram(s, idx)\n",
    "            probs.append(self.get_ngram_probability(ngram[0], ngram[1]))\n",
    "                    \n",
    "        entropy = np.log2(probs)\n",
    "        avg_entropy = -1 * (sum(entropy) / len(entropy))\n",
    "        return pow(2.0, avg_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6502f6a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d8ea744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processed corpus\n",
    "with open('LM_trainingCorpus.json') as f:\n",
    "    lm_trainingcorpus = json.load(f) # I did not keep the training corpus on a dataframe because of my computer's limited capacity \n",
    "# 15 sentences with typos\n",
    "sentences_with_typos = pd.read_csv('sentence_with_typos.txt', sep='\\t', header=0, names=['ID', 'Sentence'])\n",
    "# from the SUBTLEXus lexicon\n",
    "subtlexus = pd.read_csv('SUBTLEXus.txt', sep='\\t', na_values=['NA'], low_memory=False)\n",
    "\n",
    "# Selecting rows where 'Word' is not a string\n",
    "non_string_rows = subtlexus[~subtlexus['Word'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "# Drop NaN rows and reset index\n",
    "subtlexus = subtlexus.drop(index=non_string_rows.index[0]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d5bac9",
   "metadata": {},
   "source": [
    "## Find which token contains a typo in the given sentences\n",
    "Having a typo means that the word does not feature in SUBTLEXus. There is one and only one word containing a typo in each sentence, as defined in this way: the typo can result from the insertion/deletion/substitution of one or two characters. The generated .json file contains a simple dictionary mapping the sentence ID (as an integer) to the mistyped word (as a string).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b1caa85-3bfa-46f1-8d9b-45de973eafa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knoq is not in SubtlexUs\n",
      "reserachers is not in SubtlexUs\n",
      "grozn is not in SubtlexUs\n",
      "quolg is not in SubtlexUs\n",
      "waies is not in SubtlexUs\n",
      "wintr is not in SubtlexUs\n",
      "munors is not in SubtlexUs\n",
      "surgicaly is not in SubtlexUs\n",
      "aquire is not in SubtlexUs\n",
      "acomodate is not in SubtlexUs\n",
      "dats is not in SubtlexUs\n",
      "collegue is not in SubtlexUs\n",
      "layed is not in SubtlexUs\n",
      "cate is not in SubtlexUs\n",
      "giambic is not in SubtlexUs\n"
     ]
    }
   ],
   "source": [
    "# find mistyped words in the input sentences\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "\n",
    "# create list of lexicon tokens \n",
    "subtlexus_tok = list(subtlexus['Word'])\n",
    "\n",
    "# function to find the nearest neighbor of a target  \n",
    "def edit_distance(target):\n",
    "    \"\"\"\n",
    "    This function takes a target word as input and calculates its minimum edit distance to other words in SubtlexUS.\n",
    "\n",
    "    Args:\n",
    "        target (str): target word for which minimum edit distance is to be calculated.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the minimum edit distance and the list of words that have the same minimum edit distance.\n",
    "    \"\"\"\n",
    "    # initialize variables\n",
    "    min_edit = 100  # arbitrary large  \n",
    "    neighbors = []\n",
    "    \n",
    "    # iterate over all the tokens in subtlexus_tok\n",
    "    for word in subtlexus_tok:\n",
    "        # if the word is a valid string\n",
    "        if isinstance(word, str):\n",
    "            # remove non-alphabetic characters\n",
    "            word = re.findall(r'^[^a-zA-z]*([a-zA-Z]*)[^a-zA-z]*$', word)   \n",
    "            if word and word[0] != target:\n",
    "                # calculate edit distance\n",
    "                d = nltk.edit_distance(word[0], target)\n",
    "                if d < min_edit:\n",
    "                    min_edit = d\n",
    "                    neighbors = [word[0]]\n",
    "                elif d == min_edit:\n",
    "                    neighbors.append(word[0])\n",
    "                else:\n",
    "                    pass\n",
    "    return (min_edit, neighbors)\n",
    "\n",
    "\n",
    "def create_mistyped_dict():\n",
    "    \"\"\"\n",
    "    Creates a dictionary of mistyped words from a dataframe of sentences with typos.\n",
    "    \n",
    "    Returns:\n",
    "        mistyped_dict (dict): A dictionary where each key is a sentence ID and the value is a mistyped word.\n",
    "    \"\"\"\n",
    "    # create a dictionary to store mistyped words\n",
    "    mistyped_dict = {}  \n",
    "\n",
    "    # loop through each sentence \n",
    "    for index, row in sentences_with_typos.iterrows():\n",
    "        sentence_id = row['ID']\n",
    "        sentence = row['Sentence']\n",
    "\n",
    "        # tokenize sentence (ignoring special characters)\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        words = tokenizer.tokenize(sentence)\n",
    "\n",
    "        # loop through each word in the sentence\n",
    "        for word in words:\n",
    "            if word not in subtlexus_tok:\n",
    "                print(word, 'is not in SubtlexUs')\n",
    "                # find nearest neighbor \n",
    "                min_edit, neighbors = edit_distance(word)\n",
    "                if min_edit <= 2:\n",
    "                    mistyped_dict[sentence_id] = word\n",
    "                    # only consider 1 mistyped word per sentence\n",
    "                    break  \n",
    "                    \n",
    "    return mistyped_dict\n",
    "\n",
    "mistyped_dict = create_mistyped_dict()\n",
    "\n",
    "# store mistyped words in JSON file\n",
    "with open('task2a_PaulaAraujoRabinovich_solution.json', 'w') as f:\n",
    "    json.dump(mistyped_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b126e",
   "metadata": {},
   "source": [
    "## Find the words from SUBTLEXus with the smallest edit distance from each mistyped target\n",
    "This should return the 3 words at the smallest edit distance, sorted by edit distance. However, if there are more words at the same edit distance than the third closest, the code should include all the words at the same edit distance. Therefore, supposing that the string 'abcdef' has two neighbors at edit distance 1, and four neighbors at edit distance 2, the third closest neighbor would be at edit distance 2, but there would be other three words at the same distance and then the code should thus return six neighbors for the target string. The generated .json file contains a dictionary mapping sentence IDs (as integers) to lists of tuples, where each tuple contains first the word (as a string) and then the edit distance (as an integer), with tuples sorted by edit distance in ascending order (smallest edit distance first). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c42f26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the nearest neighbors based on edit distance\n",
    "\n",
    "def find_best_neighbors(target):\n",
    "    \"\"\"\n",
    "    Finds the 3 closest neighbors (or more, when applicable) to a given target word based on edit distance. \n",
    "\n",
    "    Args:\n",
    "        target (str): The target word we want to find the closest neighbors for.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the closest neighbors of the target word, with edit distances as keys and a list of closest neighbors as values.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    neighbors = {}  # dictionary to store all neighbors \n",
    "    \n",
    "    # for each word in the lexicon\n",
    "    for word in subtlexus_tok:\n",
    "        # if the word is a valid string\n",
    "        if isinstance(word, str):\n",
    "            # extract only alphabetic characters from the word\n",
    "            word = re.findall(r'^[^a-zA-z]*([a-zA-Z]*)[^a-zA-z]*$', word)\n",
    "            # check that the word (from subtlexus_tok) is not empty and that it is not the same as the target misspelled word \n",
    "            if word and word[0] != target:\n",
    "                # calculate the distance between word in lexicon and target word\n",
    "                d = nltk.edit_distance(word[0], target)\n",
    "                # if distance is lower than our edit distance threshold\n",
    "                if d <= 100:  \n",
    "                    # if distance is not already in our dictionary of neighbors\n",
    "                    if d not in neighbors:\n",
    "                        # create new dictionary entry \n",
    "                        neighbors[d] = [word[0]]\n",
    "                    else:\n",
    "                        # extend an existing dictionary entry\n",
    "                        neighbors[d].append(word[0])\n",
    "\n",
    "    closest_neighbors = {} # dictionary to store only the closest neighbors\n",
    "    \n",
    "    # loop through each mistyped word, and store its closest neighbors\n",
    "    for key, value in sorted(neighbors.items()):\n",
    "        closest_neighbors[key] = value\n",
    "        # if the length of the closest neighbors dictionary is already equal or greater than 3, break the loop\n",
    "        if sum(len(v) for v in closest_neighbors.values()) >= 3:\n",
    "            break  \n",
    "    \n",
    "    # the function returns the closest neighbors of one single target (one mistyped word)\n",
    "    return closest_neighbors \n",
    "\n",
    "\n",
    "# this function to apply find_best_neighbors() to all 15 mistyped words\n",
    "def get_result():\n",
    "\n",
    "    \"\"\"\n",
    "    Applies find_best_neighbors() to all 15 mistyped words, and returns a dictionary.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the closest neighbors of each mistyped word. The dictionary maps sentence IDs (as integers) to lists of tuples, \n",
    "    where each tuple contains first the word (as a string) and then the edit distance (as an integer).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    results_dict = {}\n",
    "\n",
    "    # iterate through each mistyped word (one per sentence)\n",
    "    for key, mistyped_word in mistyped_dict.items():  \n",
    "        \n",
    "        # for each mistyped word, create a dictionary of its neighbors (keys being the edit-distance)\n",
    "        closest_correct_words = find_best_neighbors(mistyped_word) \n",
    "        # example: {2: ['researchers'], 3: ['researcher', 'researches', 'reachers']}\n",
    "\n",
    "        all_tuples = []  \n",
    "        # here we will store the neighbors of the current mispelled word being processed in tuple form\n",
    "        # example: [('researchers', 2), ('researcher', 3), ('researches', 3), ('reachers', 3)]\n",
    "\n",
    "        # loop through dictionary (distance = dict key, neighbors = dict values) \n",
    "        for distance, neighbors in closest_correct_words.items(): \n",
    "            # for each potential neighbor of a given edit-distance\n",
    "            for neighbor in neighbors: \n",
    "                # create a tuple with the (correct word, edit-distance)\n",
    "                one_tuple = (neighbor, distance)\n",
    "                # store the tuple in the list (there is one list of tuples for every mispelled word)\n",
    "                all_tuples.append(one_tuple)\n",
    "        \n",
    "        results_dict[key] = all_tuples   \n",
    "\n",
    "    return results_dict\n",
    "\n",
    "edit_dist_dict = get_result()\n",
    "\n",
    "# store candidates with smallest edit distance from each mistyped target in JSON file\n",
    "with open('task3_PaulaAraujoRabinovich_solution.json', 'w') as f:\n",
    "    json.dump(edit_dist_dict, f)\n",
    "    \n",
    "#print(edit_dist_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f911707",
   "metadata": {},
   "source": [
    "## Best candidate according to frequency \n",
    "Then the list of candidate replacements found in the previous task can be used to find the best one according to candidate frequency (derived from SUBTLEXus) - if two or more candidates have the exact same frequency in SUBTLEX, then this progam chooses the one with the best edit distance. If two or more candidates at the same frequency also have the same edit distance, then it picks the one which comes first in alphabetical order. The program returns a .json file containing a simple dictionary mapping the sentence ID (as an integer) to a tuple containing the best candidate replacement (as a string) and its SUBTLEXus frequency value (as an integer).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8d0245-6c6f-40c0-9be4-1d46c74f87fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the best candidate according to SUBTLEXus frequency counts\n",
    "\n",
    "def create_freq_dict(dictionary):\n",
    "    \"\"\"\n",
    "    Creates a dictionary of frequencies for each mistyped word.\n",
    "    \n",
    "    Args:\n",
    "        dictionary (dict): The dictionary of tuples (word, edit_distance) to process\n",
    "\n",
    "    Returns:\n",
    "    freq_dict (dict): A dictionary where the keys are the sentence ID and \n",
    "    the values are lists of tuples where each tuple contains a candidate word \n",
    "    and its frequency count in the SubtlexUs corpus.\n",
    "    \"\"\"\n",
    "    freq_dict = {}\n",
    "    \n",
    "    for i in range(1,16):\n",
    "        # retrieve candidate words for the current mistyped word being processed\n",
    "        candidates = dictionary[i]\n",
    "        # for each tuple in the candidate list\n",
    "        for tup in candidates:\n",
    "            # grab word\n",
    "            word = tup[0]\n",
    "            # find word frequency\n",
    "            target_row = subtlexus[subtlexus['Word'] == word]\n",
    "            freq = target_row['FREQcount'].values[0]\n",
    "            # if row ID is not already in our dictionary of frequencies\n",
    "            if i not in freq_dict:\n",
    "                # create new dictionary entry \n",
    "                freq_dict[i] = [(word, freq)]\n",
    "            else:\n",
    "                # extend an existing dictionary entry\n",
    "                freq_dict[i].append((word, freq))   \n",
    "                \n",
    "    return freq_dict\n",
    "\n",
    "def find_tuple(string, dictionary):\n",
    "    \"\"\"\n",
    "    Returns the tuple containing the input string from the given dictionary of tuples.\n",
    "\n",
    "    Args:\n",
    "        string (str): The string to search for in the tuples.\n",
    "        dictionary (dict): The dictionary of tuples to search in.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The tuple containing the input string in its first positional argument.\n",
    "    \"\"\"\n",
    "    for value in dictionary.values():\n",
    "        for tpl in value:\n",
    "            if tpl[0] == string:\n",
    "                return tpl\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_best_word(i, freq_dict, edit_distance_dict):\n",
    "    \"\"\"\n",
    "    Finds the tuple containing the ('best_word', frequency) for a sentence of index (i)\n",
    "    One of 3 possible tuples can be returned:\n",
    "        1) One with highest freq count\n",
    "        2) One with highest freq count AND lowest edit distance (in case there are >1 words with highest freq count)\n",
    "        3) One with highest freq count AND lowest edit distance AND highest in the alphabet is returned \n",
    "           (in case there are >1 words with highest freq count and lowest edit-dist)\n",
    "\n",
    "    Args:\n",
    "        i (integer): Sentence ID.\n",
    "        freq_dict (dictionary): Words with frequency counts.\n",
    "        edit_distance_dict (dictionary): Words with their edit-distance.\n",
    "\n",
    "    Returns:\n",
    "        tuple: ('best_word', frequency)\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'Processing {freq_dict[i]} for sentence {i}')\n",
    "    # Determine the max value in the frequency dictionary\n",
    "    max_value = max(freq_dict[i], key=lambda tpl: tpl[1])[1]\n",
    "    print(f'\\t Max frequency value is: {max_value} ')\n",
    "    \n",
    "    # Filter to find all tuples with the max value in terms of frequency counts\n",
    "    max_tuples = [tpl for tpl in freq_dict[i] if tpl[1] == max_value] \n",
    "    print(f'\\t Max tuple(s) is: {max_tuples} ')\n",
    "    \n",
    "    # fetch all words corresponding to the max frequencies\n",
    "    maxfreq_words = [tpl[0] for tpl in max_tuples]\n",
    "    print(f'\\t Most frequent word(s) is: {maxfreq_words} ', end=\"\\n\")\n",
    "    \n",
    "    # IF there are multiple words with the same max frequency\n",
    "    if len(max_tuples) > 1:\n",
    "        print(f'\\t There are multiple words with the same max frequency')\n",
    "        # store relevant tuples (containing the words with max frequency)\n",
    "        relevant_tuples = []\n",
    "        for word in maxfreq_words:\n",
    "            for tpl in edit_distance_dict[i]: # from the edit-distance dict\n",
    "                if tpl[0] == word:\n",
    "                    relevant_tuples.append(tpl)\n",
    "                    \n",
    "        # Determine the min value of these words in the edit-distance dictionary\n",
    "        min_value = min(relevant_tuples, key=lambda tpl: tpl[1])[1]\n",
    "        print(f'\\t\\t Minimum edit-distance value for word(s) with max frequency is: {min_value} ')\n",
    "     \n",
    "        # Filter to find all relevant_tuples with the minimum value in terms of the edit-distance\n",
    "        min_tuples = [tpl for tpl in relevant_tuples if tpl[1] == min_value] \n",
    "        print(f'\\t\\t Minimum edit-distance tuple(s) for word(s) with max frequency is: {min_tuples} ')\n",
    "        \n",
    "        # fetch all words corresponding to the max frequencies and minimum edit-distance\n",
    "        maxfreq_min_dist = [tpl[0] for tpl in min_tuples]\n",
    "        \n",
    "        # IF there are multiple words with the same max frequency AND minimum edit-distance\n",
    "        if len(maxfreq_min_dist) > 1:\n",
    "            print(f'\\t\\t There are multiple words with equal frequency counts and edit-distance: {min_tuples}')\n",
    "            # Select tuple (from the list) that comes first in the alphabetical order\n",
    "            alphabetical_max = min(min_tuples, key=lambda x: x[0])\n",
    "            # alphabetical_max contains the tuple from the edit-distance dictionary -> (word, edit-distance)\n",
    "            # we need the right tuple that contains the frequency counts -> (word, counts)\n",
    "            \n",
    "            # grab tuple with frequency counts \n",
    "            best_word = find_tuple(alphabetical_max[0], freq_dict)\n",
    "            print(f'\\t\\t In this case, the best candidate is the one that comes first in the alphabetical order: {best_word}')\n",
    "            return best_word  \n",
    "        \n",
    "        elif len(maxfreq_min_dist) == 1:\n",
    "            # min_tuples[0]  contains the tuple from the edit-distance dictionary -> (word, edit-distance)\n",
    "            # we need the right tuple that contains the frequency counts -> (word, counts)\n",
    "            \n",
    "            # grab tuple with frequency counts \n",
    "            best_word = find_tuple(min_tuples[0][0], freq_dict)\n",
    "            return best_word\n",
    "        \n",
    "    elif len(max_tuples) == 1:\n",
    "        # max_tuples[0] already contains the necessary information in the form of ('best_word', frequency)\n",
    "        return max_tuples[0] #  \n",
    "    \n",
    "    else:\n",
    "        print('max_tuples is of length 0')\n",
    "        \n",
    "def get_best_candidates(edit_dist_dict):\n",
    "    \"\"\"\n",
    "    Finds the best candidate word for every one of the 15 mistyped words using frequency counts.\n",
    "\n",
    "    Args:\n",
    "        edit_dist_dict (dictionary): Dictionary with sentence IDs as dict keys, \n",
    "        and all candidate tuples as dict values, in the form of (word, edit-distance)\n",
    "\n",
    "    Returns:\n",
    "        dictionary: Dictionary with sentence IDs as keys, and the corresponding \n",
    "        best candidate tuple (word, frequency) as dict value.\n",
    "         \n",
    "    \"\"\"\n",
    "    \n",
    "    best_candidates = {}\n",
    "    # create dictionary to store frequency counts of words\n",
    "    freq_dict = create_freq_dict(edit_dist_dict)\n",
    "    # loop through each sentence to get its best word candidate\n",
    "    for i in range(1,16):\n",
    "        best_tuple = get_best_word(i, freq_dict, edit_dist_dict)\n",
    "        # store the best candidate for sentence i\n",
    "        best_candidates[i] = best_tuple\n",
    "        \n",
    "    return best_candidates\n",
    "\n",
    "def convert(o):\n",
    "    if isinstance(o, np.int64): return int(o)  \n",
    "    raise TypeError\n",
    "    \n",
    "best_frequency_candidates = get_best_candidates(edit_dist_dict)\n",
    "\n",
    "# store best candidates based on frequency in JSON file \n",
    "with open('task4_PaulaAraujoRabinovich_solution.json', 'w') as f:\n",
    "    json.dump(best_frequency_candidates, f, default=convert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1024d5b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Best candidate according to perplexity\n",
    "\n",
    "We can use the list of candidate replacements to find the best one according to its perplexity under a statistical language model of order 3 implemented using a Markov Chain with add-k smoothing (k=0.01) and estimated using the given corpus. If two or more candidates have the exact same perplexity in the input sentence, we can choose the one with the best edit distance. If two or more candidates at the same perplexity also have the same edit distance, then we can follow alphabetical order. The program returns a .json file containing a simple dictionary mapping the sentence ID (as an integer) to a tuple containing the best candidate replacement (as a string) and its perplexity under the specified language model (as a float). Not all candidate replacements might appear in the LM training corpus: in such casse the program won't map such candidate replacements to the UNK string though, or else the perplexity estimate would not reflect the specific candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4bc2e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [('know', 1002.7288999720108), ('knot', 5639.354069762032), ('knob', 8575.278397480908)] for sentence 1\n",
      "\t Min perplexity value is: 1002.7288999720108 \n",
      "\t Min tuple(s) is: [('know', 1002.7288999720108)] \n",
      "\t Lowest perplexity word(s) is: ['know'] \n",
      "Processing [('researchers', 14475.415924638553), ('researcher', 14476.248088113443), ('researches', 14475.415924638553)] for sentence 2\n",
      "\t Min perplexity value is: 14475.415924638553 \n",
      "\t Min tuple(s) is: [('researchers', 14475.415924638553), ('researches', 14475.415924638553)] \n",
      "\t Lowest perplexity word(s) is: ['researchers', 'researches'] \n",
      "\t There are multiple words with the same min perplexity: [('researchers', 14475.415924638553), ('researches', 14475.415924638553)]\n",
      "\t\t Minimum edit-distance value for word(s) with min perplexity is: 2 \n",
      "\t\t Minimum edit-distance tuple(s) for word(s) with min perplexity is: [('researchers', 2)] \n",
      "Processing [('grown', 1861.3599787842493), ('groin', 1858.1347148669051), ('groan', 1858.8593594720237)] for sentence 3\n",
      "\t Min perplexity value is: 1858.1347148669051 \n",
      "\t Min tuple(s) is: [('groin', 1858.1347148669051)] \n",
      "\t Lowest perplexity word(s) is: ['groin'] \n",
      "Processing [('quote', 16820.32265338231), ('quota', 28937.539844403862), ('quo', 28933.1915634287), ('quilt', 19307.938029302248), ('quell', 28933.1915634287), ('qualm', 28946.21295587551)] for sentence 4\n",
      "\t Min perplexity value is: 16820.32265338231 \n",
      "\t Min tuple(s) is: [('quote', 16820.32265338231)] \n",
      "\t Lowest perplexity word(s) is: ['quote'] \n",
      "Processing [('waves', 1220.1171486647056), ('wakes', 1832.2622055583065), ('waits', 1831.7334704549087), ('wages', 1827.260259071301), ('wails', 1827.3976276540632), ('wares', 1827.260259071301), ('waxes', 1827.5350065638286), ('wades', 1827.260259071301), ('waives', 1827.260259071301), ('waifs', 1827.260259071301)] for sentence 5\n",
      "\t Min perplexity value is: 1220.1171486647056 \n",
      "\t Min tuple(s) is: [('waves', 1220.1171486647056)] \n",
      "\t Lowest perplexity word(s) is: ['waves'] \n",
      "Processing [('winter', 2147.773768366897), ('wintry', 8219.960238645888), ('with', 5457.273700422706), ('want', 5541.212585560772), ('into', 5958.714744103964), ('went', 5722.395012400671), ('wants', 6009.844237624907), ('win', 4333.3925001443), ('wind', 1857.4043179891894), ('wine', 2409.212747441707), ('winner', 3936.8885081093167), ('wins', 6625.27510054141), ('wings', 3415.748402725805), ('wing', 3888.4758092796073), ('minor', 6980.954364550521), ('hint', 4120.202047767535), ('diner', 3806.262556587309), ('winds', 2950.4109952050853), ('ninth', 5070.030649548264), ('wit', 6515.402135742955), ('mint', 4659.915538373562), ('witty', 6969.735041298211), ('wits', 6612.058895249942), ('wiser', 4017.626054764363), ('pint', 7226.796040151702), ('wink', 4303.658281411412), ('finer', 7650.667976909389), ('wider', 6430.411415075977), ('windy', 7227.261882365566), ('intro', 8217.303731714712), ('hints', 6901.660003497013), ('wiener', 7638.858131216718), ('mints', 5027.70527463821), ('wilt', 7103.6521532220395), ('wines', 7849.330725941123), ('miner', 5267.823418440419), ('liner', 5010.190986954605), ('pints', 8214.099970533987), ('lint', 7225.395985693248), ('winch', 5128.884178501296), ('wont', 7244.6596567006945), ('width', 11173.298700372055), ('Pinto', 8215.16983554193), ('wino', 6932.666699270305), ('winks', 7160.9815870800185), ('wiper', 8215.16983554193), ('minty', 11173.298700372055), ('winos', 8215.702472739662), ('inter', 7495.139699098085), ('tint', 7638.360466243728), ('dinar', 11172.570769665605), ('aint', 5617.928533994305), ('whiner', 6443.882361054692), ('wined', 7296.128372802177), ('winery', 11172.570769665605), ('wince', 7494.163603916336), ('Pinta', 11172.570769665605), ('entr', 11172.570769665605), ('winder', 11172.570769665605)] for sentence 6\n",
      "\t Min perplexity value is: 1857.4043179891894 \n",
      "\t Min tuple(s) is: [('wind', 1857.4043179891894)] \n",
      "\t Lowest perplexity word(s) is: ['wind'] \n",
      "Processing [('minors', 14199.607351154302), ('minor', 14199.607351154302), ('rumors', 9338.085472292762), ('honors', 14199.607351154302), ('manor', 14199.607351154302), ('majors', 14199.607351154302), ('miners', 14202.129404060332), ('jurors', 9336.433652424052), ('donors', 14200.86893749825), ('tumors', 14199.607351154302), ('moors', 14199.607351154302), ('tutors', 14199.607351154302), ('juniors', 14199.607351154302), ('mucous', 14199.607351154302), ('monos', 14199.607351154302), ('mayors', 14199.607351154302), ('mentors', 14199.607351154302), ('tenors', 14199.607351154302), ('manos', 14199.607351154302)] for sentence 7\n",
      "\t Min perplexity value is: 9336.433652424052 \n",
      "\t Min tuple(s) is: [('jurors', 9336.433652424052)] \n",
      "\t Lowest perplexity word(s) is: ['jurors'] \n",
      "Processing [('surgical', 31362.11127051458), ('surgically', 34597.21194715582), ('strictly', 31362.11127051458), ('survival', 18918.657040618258)] for sentence 8\n",
      "\t Min perplexity value is: 18918.657040618258 \n",
      "\t Min tuple(s) is: [('survival', 18918.657040618258)] \n",
      "\t Lowest perplexity word(s) is: ['survival'] \n",
      "Processing [('acquire', 2192.659500352165), ('Squire', 2190.000743626073), ('quire', 2190.000743626073)] for sentence 9\n",
      "\t Min perplexity value is: 2190.000743626073 \n",
      "\t Min tuple(s) is: [('Squire', 2190.000743626073), ('quire', 2190.000743626073)] \n",
      "\t Lowest perplexity word(s) is: ['Squire', 'quire'] \n",
      "\t There are multiple words with the same min perplexity: [('Squire', 2190.000743626073), ('quire', 2190.000743626073)]\n",
      "\t\t Minimum edit-distance value for word(s) with min perplexity is: 1 \n",
      "\t\t Minimum edit-distance tuple(s) for word(s) with min perplexity is: [('Squire', 1), ('quire', 1)] \n",
      "\t\t There are multiple words with same min perplexity and edit-distance: [('Squire', 1), ('quire', 1)]\n",
      "\t\tt Keep the word that comes first in the alphabetical order: ('Squire', 1)\n",
      "Processing [('accomodate', 1897.8469004552046), ('accommodate', 2767.2030206738777), ('accommodated', 2767.2030206738777)] for sentence 10\n",
      "\t Min perplexity value is: 1897.8469004552046 \n",
      "\t Min tuple(s) is: [('accomodate', 1897.8469004552046)] \n",
      "\t Lowest perplexity word(s) is: ['accomodate'] \n",
      "Processing [('days', 3797.947657551638), ('date', 2274.1923660774823), ('dates', 4995.018376453822), ('data', 5185.0393776805895), ('cats', 3898.330002794193), ('eats', 7488.765892329181), ('rats', 7510.381791460462), ('hats', 5394.330707782133), ('bats', 7503.253126665158), ('dots', 5396.933103757329), ('dads', 7490.854135841328), ('oats', 7490.332784105784), ('darts', 7493.972380181253), ('mats', 5390.9755470007585), ('fats', 7489.810960201678), ('pats', 3689.5422881210898), ('dams', 7489.810960201678), ('dais', 5397.248261482091), ('tats', 7489.288663239995), ('Lats', 7488.765892329181), ('dabs', 7491.895426350097)] for sentence 11\n",
      "\t Min perplexity value is: 2274.1923660774823 \n",
      "\t Min tuple(s) is: [('date', 2274.1923660774823)] \n",
      "\t Lowest perplexity word(s) is: ['date'] \n",
      "Processing [('college', 1307.6444147113198), ('colleague', 888.999722318408), ('colleagues', 1309.3597557162418), ('colleges', 1303.8538389114717), ('collage', 1303.7476585946565)] for sentence 12\n",
      "\t Min perplexity value is: 888.999722318408 \n",
      "\t Min tuple(s) is: [('colleague', 888.999722318408)] \n",
      "\t Lowest perplexity word(s) is: ['colleague'] \n",
      "Processing [('played', 6631.220181040861), ('layer', 6625.174599990876), ('laced', 6626.791073901103), ('Fayed', 6625.174599990876), ('flayed', 6625.579273379231), ('slayed', 6625.174599990876)] for sentence 13\n",
      "\t Min perplexity value is: 6625.174599990876 \n",
      "\t Min tuple(s) is: [('layer', 6625.174599990876), ('Fayed', 6625.174599990876), ('slayed', 6625.174599990876)] \n",
      "\t Lowest perplexity word(s) is: ['layer', 'Fayed', 'slayed'] \n",
      "\t There are multiple words with the same min perplexity: [('layer', 6625.174599990876), ('Fayed', 6625.174599990876), ('slayed', 6625.174599990876)]\n",
      "\t\t Minimum edit-distance value for word(s) with min perplexity is: 1 \n",
      "\t\t Minimum edit-distance tuple(s) for word(s) with min perplexity is: [('layer', 1), ('Fayed', 1), ('slayed', 1)] \n",
      "\t\t There are multiple words with same min perplexity and edit-distance: [('layer', 1), ('Fayed', 1), ('slayed', 1)]\n",
      "\t\tt Keep the word that comes first in the alphabetical order: ('Fayed', 1)\n",
      "Processing [('care', 11882.810558938134), ('came', 26272.17497899488), ('late', 26270.200054989145), ('case', 26270.200054989145), ('hate', 26270.200054989145), ('date', 26270.200054989145), ('cute', 26270.200054989145), ('cat', 26270.200054989145), ('ate', 26270.200054989145), ('cake', 26270.200054989145), ('rate', 26270.200054989145), ('gate', 26270.200054989145), ('fate', 26270.200054989145), ('mate', 26270.200054989145), ('cage', 26270.200054989145), ('cats', 26270.200054989145), ('cave', 26270.200054989145), ('cafe', 26270.200054989145), ('cane', 26270.200054989145), ('crate', 26270.200054989145), ('cater', 26270.200054989145), ('carte', 26270.200054989145), ('cite', 26270.200054989145), ('Tate', 26270.200054989145), ('pate', 26270.200054989145), ('caste', 26270.200054989145), ('sate', 26270.200054989145), ('bate', 26270.200054989145)] for sentence 14\n",
      "\t Min perplexity value is: 11882.810558938134 \n",
      "\t Min tuple(s) is: [('care', 11882.810558938134)] \n",
      "\t Lowest perplexity word(s) is: ['care'] \n",
      "Processing [('iambic', 31291.168009478923), ('gambit', 123769.33625520598), ('limbic', 123769.33625520598)] for sentence 15\n",
      "\t Min perplexity value is: 31291.168009478923 \n",
      "\t Min tuple(s) is: [('iambic', 31291.168009478923)] \n",
      "\t Lowest perplexity word(s) is: ['iambic'] \n"
     ]
    }
   ],
   "source": [
    "# pick the best candidate according to perplexity under the given language model\n",
    "\n",
    "# create a language model based on tri-grams using smoothing constant k=0.01\n",
    "trigram_model = LM(lm_trainingcorpus, ngram_size=3, bos='+', eos='#', k=0.01)\n",
    "trigram_model.update_counts()\n",
    "\n",
    "def create_variations():\n",
    "    \"\"\"\n",
    "    Creates a dictionary with all possible sentence variations by replacing the mistyped words. \n",
    "\n",
    "    Returns:\n",
    "        A dictionary where the keys are sentence IDs and the values are lists of all possible sentence variations.\n",
    "    \"\"\"\n",
    "    all_sentences = list(sentences_with_typos['Sentence'])\n",
    "    sentence_variations = {}\n",
    "    \n",
    "    # for each sentence ID and mistyped word  \n",
    "    for sent_id, mistyped_word in mistyped_dict.items():\n",
    "        # grab the original sentence string from the all_sentences list \n",
    "        original_sentence = all_sentences[sent_id-1]\n",
    "        # grab list of neighbors for the sentence being processed\n",
    "        neighbors = [elem[0] for elem in edit_dist_dict[sent_id]]\n",
    "        \n",
    "        # store only those neighbors that appear in our LM training vocab\n",
    "        neighbors_invocab = []\n",
    "        # loop through all neighbors\n",
    "        for neighbor in neighbors:\n",
    "            # only keep word candidates that appear in the LM training corpus \n",
    "            if neighbor in trigram_model.vocab:\n",
    "                neighbors_invocab.append(neighbor)\n",
    "                \n",
    "        # create a variation of the original sentence by replacing the mistyped word with each neighbor in the LM vocabulary\n",
    "        variations = [original_sentence.replace(mistyped_word, neighbor) for neighbor in neighbors_invocab]\n",
    "        sentence_variations[sent_id] = variations\n",
    "\n",
    "    return sentence_variations\n",
    "\n",
    "\n",
    "\n",
    "def create_perplexity_dict(sentence_variations_dict):\n",
    "    \"\"\"\n",
    "    Create a dictionary of perplexity scores for candidate replacements\n",
    "\n",
    "    Parameters:\n",
    "    dictionary (dict): a dictionary where keys are sentence IDs and values \n",
    "    are a list of variations for each sentence.\n",
    "\n",
    "    Returns:\n",
    "    dict: a dictionary where keys are sentence IDs and values are a tuple containing \n",
    "    the neighbor word involved in the perplexity calculation and its corresponding perplexity score.\n",
    "    \"\"\"    \n",
    "    \n",
    "    perplexity_dict = {} \n",
    "    # keys: sentence ID\n",
    "    # values: (neighbor, perplexity)  \n",
    "    \n",
    "    # iterate through all main sentences \n",
    "    for i in range(1,16):\n",
    "        \n",
    "        # grab all sentence variations for the ith original sentence\n",
    "        sentences = sentence_variations_dict[i]\n",
    "        # store variations of sentence i (tokenized) in a list\n",
    "        sentences_tokenized = []\n",
    "\n",
    "        # tokenize sentence variations (for each sentence i being processed)\n",
    "        for sentence in sentences:\n",
    "            sentences_tokenized.append(nltk.wordpunct_tokenize(sentence))   \n",
    "\n",
    "        # iterate through each sentence variation of sentence i\n",
    "        for j in range(len(sentences_tokenized)):\n",
    "\n",
    "            # identify the specific relevant neighbor involved in this particular sentence variation \n",
    "            all_neighbors = edit_dist_dict[i] # grab all neighbor words for sentence i  \n",
    "            relevant_word = [t[0] for t in all_neighbors][j] # grab neighbor word j \n",
    "\n",
    "            # calculate perplexity for each sentence\n",
    "            perplexity = trigram_model.perplexity(sentences_tokenized[j])\n",
    "\n",
    "            # if row ID is not already in our dictionary of frequencies\n",
    "            if i not in perplexity_dict:\n",
    "                # create new dictionary entry \n",
    "                perplexity_dict[i] = [(relevant_word, perplexity)]\n",
    "            else:\n",
    "                # extend an existing dictionary entry\n",
    "                perplexity_dict[i].append((relevant_word, perplexity))  \n",
    "                \n",
    "    return perplexity_dict\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "def get_best_word(i, perplexity_dict, edit_dist_dict):\n",
    "    \"\"\"\n",
    "    Finds the tuple containing the ('best_word', perplexity) for a sentence of index (i)\n",
    "    One of 3 possible tuples can be returned:\n",
    "        1) One with lowest perplexity\n",
    "        2) One with lowest perplexity AND lowest edit distance (in case there are >1 words with highest freq count)\n",
    "        3) One with lowest perplexity AND lowest edit distance AND highest in the alphabet is returned \n",
    "           (in case there are >1 words with highest freq count and lowest edit-dist)\n",
    "\n",
    "    Args:\n",
    "        i (integer): Sentence ID.\n",
    "        perplexity_dict (dictionary): Words with their perplexity.\n",
    "        edit_distance_dict (dictionary): Words with their edit-distance.\n",
    "\n",
    "    Returns:\n",
    "        tuple: ('best_word', perplexity)\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'Processing {perplexity_dict[i]} for sentence {i}')\n",
    "    # Determine the min value in the perplexity dictionary\n",
    "    min_value = min(perplexity_dict[i], key=lambda tpl: tpl[1])[1]\n",
    "    print(f'\\t Min perplexity value is: {min_value} ')\n",
    "    \n",
    "    # Filter to find all tuples with the min value in terms of perplexity\n",
    "    min_tuples = [tpl for tpl in perplexity_dict[i] if tpl[1] == min_value] \n",
    "    print(f'\\t Min tuple(s) is: {min_tuples} ')\n",
    "    \n",
    "    # fetch all words corresponding to the min perplexity\n",
    "    min_words = [tpl[0] for tpl in min_tuples]\n",
    "    print(f'\\t Lowest perplexity word(s) is: {min_words} ', end=\"\\n\")\n",
    "    \n",
    "    # IF there are multiple words with the same min perplexity\n",
    "    if len(min_tuples) > 1:\n",
    "        print(f'\\t There are multiple words with the same min perplexity: {min_tuples}')\n",
    "        # store relevant tuples (containing the words with min perplexity)\n",
    "        relevant_tuples = []\n",
    "        for word in min_words:\n",
    "            for tpl in edit_dist_dict[i]: # from the edit-distance dict\n",
    "                if tpl[0] == word:\n",
    "                    relevant_tuples.append(tpl)\n",
    "                    \n",
    "        # Determine the min value of these words in the edit-distance dictionary\n",
    "        min_dist_value = min(relevant_tuples, key=lambda tpl: tpl[1])[1]\n",
    "        print(f'\\t\\t Minimum edit-distance value for word(s) with min perplexity is: {min_dist_value} ')\n",
    "     \n",
    "        # Filter to find all relevant_tuples with the minimum value in terms of the edit-distance\n",
    "        min_dist_tuples = [tpl for tpl in relevant_tuples if tpl[1] == min_dist_value] \n",
    "        print(f'\\t\\t Minimum edit-distance tuple(s) for word(s) with min perplexity is: {min_dist_tuples} ')\n",
    "        \n",
    "        # fetch all words corresponding to the minimum perplexity and minimum edit-distance\n",
    "        min_prp_min_dist = [tpl[0] for tpl in min_dist_tuples]\n",
    "        \n",
    "        # IF there are multiple words with the same minimum perplexity AND minimum edit-distance\n",
    "        if len(min_prp_min_dist) > 1:\n",
    "            print(f'\\t\\t There are multiple words with same min perplexity and edit-distance: {min_dist_tuples}')\n",
    "            # Select tuple (from the list) that comes first in the alphabetical order\n",
    "            alphabetical_max = min(min_dist_tuples, key=lambda x: x[0])\n",
    "            print(f'\\t\\tt Keep the word that comes first in the alphabetical order: {alphabetical_max}')\n",
    "            # alphabetical_max contains the tuple from the edit-distance dictionary -> (word, edit-distance)\n",
    "            # we need the right tuple that contains the perplexity -> (word, perplexity)\n",
    "            \n",
    "            # grab tuple with perplexity\n",
    "            best_word = find_tuple(alphabetical_max[0], perplexity_dict)\n",
    "            return best_word  # 3\n",
    "        \n",
    "        elif len(min_prp_min_dist) == 1:\n",
    "            # min_dist_tuples[0]  contains the tuple from the edit-distance dictionary -> (word, edit-distance)\n",
    "            # we need the right tuple that contains the perplexity -> (word, perplexity)\n",
    "            \n",
    "            # grab tuple with perplexity\n",
    "            best_word = find_tuple(min_dist_tuples[0][0], perplexity_dict)\n",
    "            return best_word # 2\n",
    "        \n",
    "    elif len(min_tuples) == 1:\n",
    "        # min_tuples[0] already contains the necessary information in the form of ('best_word', frequency)\n",
    "        return min_tuples[0] # 1\n",
    "    \n",
    "    else:\n",
    "        print('max_tuples is of length 0')\n",
    "        \n",
    "        \n",
    "def get_best_candidates(edit_dist_dict):\n",
    "    \"\"\"\n",
    "    Finds the best candidate word for every one of the 15 mistyped words using perplexity.\n",
    "\n",
    "    Args:\n",
    "        edit_dist_dict (dictionary): Dictionary with sentence IDs as dict keys, \n",
    "        and all candidate tuples as dict values, in the form of (word, edit-distance)\n",
    "\n",
    "    Returns:\n",
    "        dictionary: Dictionary with sentence IDs as keys, and the corresponding \n",
    "        best candidate tuple (word, frequency) as dict value.\n",
    "         \n",
    "    \"\"\"\n",
    "    # produce and store all sentence variations using candidate words\n",
    "    sentence_variations = create_variations()\n",
    "     # create dictionary to store perplexity scores of candidate words\n",
    "    perplexity_dict = create_perplexity_dict(sentence_variations)\n",
    "\n",
    "    best_candidates = {}\n",
    "\n",
    "    # loop through each sentence to get its best word candidate\n",
    "    for i in range(1,16):\n",
    "        best_tuple = get_best_word(i, perplexity_dict, edit_dist_dict)\n",
    "        # store the best candidate for sentence i\n",
    "        best_candidates[i] = best_tuple\n",
    "        \n",
    "    return best_candidates\n",
    "\n",
    "best_perplexity_candidates = get_best_candidates(edit_dist_dict)\n",
    "\n",
    "# store best candidates based on perplexity in JSON file\n",
    "with open('task5_PaulaAraujoRabinovich_solution.json', 'w') as f:\n",
    "    json.dump(best_perplexity_candidates, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8513369b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "**Candidate replacements found when considering <u>frequency</u>:**\n",
    "1. did you <font color='green'>know</font> that our fiends had a baby? \n",
    "2. I red that <font color='green'>researchers</font> managed to deviate the orbit of a comet with a satellite.\n",
    "3. I could not help but <font color='red'>grown</font> in frustration when my computer trashed right before I finished my project.\n",
    "4. I mate a <font color='red'>quote</font> from old clothes for my newborn nephew.\n",
    "5. he <font color='green'>waves</font> his wand to get the attention of the waiter.\n",
    "6. the roofs of the old house needed to he repaired before the <font color='red'>with</font>\n",
    "7. <font color='orange'>minor</font> are not allowed to purchase cigarettes or alcohol.\n",
    "8. the tumor was remove <font color='red'>strictly</font>\n",
    "9. they <font color='red'>Squire</font> the company in order to expand their business.\n",
    "10. the hotel was able to <font color='green'>accommodate</font> all of our needs.\n",
    "11. I marked the <font color='green'>days</font> on my calendar so I would not forger.\n",
    "12. I asked my <font color='orange'>college</font> for there opinion on the matter.\n",
    "13. due to the economic situation, main employees were <font color='red'>played</font> off from their jobs.\n",
    "14. I was refered to the specialist by my primary <font color='green'>care</font> physician.\n",
    "15. many poets wrote in <font color='red'>gambit</font> pentameter.\n",
    "\n",
    "*Outcome of frequency-based method:* There are 6 sentences out of 15 where a strictly accurate correction for the mistyped word was identified (40% accuracy).\n",
    "\n",
    "**Candidate replacements found when considering <u>perplexity</u>:**\n",
    "1. did you <font color='green'>know</font> that our fiends had a baby? \n",
    "2. I red that <font color='green'>researchers</font> managed to deviate the orbit of a comet with a satellite.\n",
    "3. I could not help but <font color='red'>groin</font> in frustration when my computer trashed right before I finished my project.\n",
    "4. I mate a <font color='red'>quote</font> from old clothes for my newborn nephew.\n",
    "5. he <font color='green'>waves</font> his wand to get the attention of the waiter.\n",
    "6. the roofs of the old house needed to he repaired before the <font color='red'>wind</font>\n",
    "7. <font color='red'>jurors</font> are not allowed to purchase cigarettes or alcohol.\n",
    "8. the tumor was remove <font color='red'>survival</font>\n",
    "9. they <font color='red'>Squire</font> the company in order to expand their business.\n",
    "10. the hotel was able to <font color='red'>accomodate</font> all of our needs.\n",
    "11. I marked the <font color='green'>date</font> on my calendar so I would not forger.\n",
    "12. I asked my <font color='orange'>colleague</font> for there opinion on the matter.\n",
    "13. due to the economic situation, main employees were <font color='red'>Fayed</font> off from their jobs.\n",
    "14. I was refered to the specialist by my primary <font color='green'>care</font> physician.\n",
    "15. many poets wrote in <font color='green'>iambic</font> pentameter.\n",
    "\n",
    "*Outcome of perplexity-based method:* There are 6 sentences out of 15 where a strictly accurate correction for the mistyped word was identified (40% accuracy).\n",
    "\n",
    "**Which are better? Do they match what I consider to be the right replacement?**\n",
    "\n",
    "In the section below, the corrected sentences are compared based on perplexity and frequency, with a focus only on the mistyped words of interest.\n",
    "\n",
    "Sentences 1, 2, 5, and 14:\n",
    "- Both approaches find the correct candidate replacements for these sentences, namely: \"know\", \"researchers\", \"waves\" and \"care\".\n",
    "\n",
    "Sentence 3:\n",
    "- Perplexity: groin\n",
    "- Frequency: grown\n",
    "- Both versions provide incorrect replacements for \"grozn\". The right correction should be \"groan\".\n",
    "\n",
    "Sentence 4:\n",
    "- Perplexity: quote\n",
    "- Frequency: quote\n",
    "- Both versions suggest \"quote\" as the replacement for \"quolg,\" which is incorrect. The right correction should be \"quilt\".\n",
    "\n",
    "Sentence 6:\n",
    "- Perplexity: wind\n",
    "- Frequency: with\n",
    "- Both versions provide incorrect replacements for \"wintr.\" The correct replacement should be \"winter.\"\n",
    "\n",
    "Sentence 7:\n",
    "- Perplexity: jurors\n",
    "- Frequency: minor\n",
    "- The frequency-based version is better, as \"minor\" is closest to the word \"minors\", which is the correct replacement for \"munors\". The reason for the perplexity-based method's incorrect candidate selection for this sentence may be attributed to its analysis of the sentence's context. In this case, the language model might have associated the context with a legal scenario, causing it to predict \"jurors\" instead of \"minor\". The frequency-based method, on the other hand, does not rely on context and predicts based on the similarity of the misspelled word to the correct word, which led it to choose \"minor\".\n",
    "\n",
    "Sentence 8:\n",
    "- Perplexity: survival\n",
    "- Frequency: strictly\n",
    "- Both versions are incorrect; the right correction for \"surgicaly\" should be \"surgically.\"\n",
    "\n",
    "Sentence 9:\n",
    "- Perplexity: Squire\n",
    "- Frequency: Squire\n",
    "- Both versions suggest \"Squire\" as the replacement for \"aquire,\" which is incorrect. The right correction should be \"acquire.\"\n",
    "\n",
    "Sentence 10:\n",
    "- Perplexity: accomodate\n",
    "- Frequency: accommodate\n",
    "- The frequency-based version is better, as \"accommodate\" is the correct replacement for \"acomodate\". This suggests that the frequency-based method may have had an advantage in this case, since the word \"accomodate\" is a common misspelling of \"accommodate\". Possibly, the language model could contain noise, inconsistencies, and/or misspellings. The presence of such errors in the training data might have influenced the predictions for candidate replacement, leading the program to choose the incorrect spelling in the perplexity-based method. \n",
    "\n",
    "Sentence 11:\n",
    "- Perplexity: date\n",
    "- Frequency: days\n",
    "- The right correction for \"dats\" is ambiguous; it could be either \"date\" or \"days.\" Both options are valid replacements.\n",
    "\n",
    "Sentence 12:\n",
    "- Perplexity: colleague\n",
    "- Frequency: college\n",
    "- Both versions seem somewhat incorrect. However, the perplexity-based version (\"colleague\") sounds slightly better, as it is likely that the author intended to write \"colleagues\" (in plural) in this sentence, given  the mispelled possessive pronoun \"their\" (typed as \"there\").\n",
    "\n",
    "Sentence 13:\n",
    "- Perplexity: Fayed\n",
    "- Frequency: played\n",
    "- Both versions are incorrect; the right correction for \"layed\" should be \"laid.\"\n",
    "\n",
    "Sentence 15:\n",
    "- Perplexity: iambic\n",
    "- Frequency: gambit\n",
    "- The perplexity-based version is better, as \"iambic\" is the correct replacement for \"gambit\". This may be because the perplexity-based method employs a statistical language model that considers the context in which a word is used. In this case, the word \"pentameter\" strongly suggests that the correct word is \"iambic\", since \"iambic pentameter\" is a well-known poetic form, while \"gambit pentameter\" is not a commonly used term. The frequency-based method, on the other hand, may not have taken into account this contextual information and relied solely on the frequency of the individual words.\n",
    "\n",
    "In summary, both perplexity-based and frequency-based corrections have their limitations and are not infallible. In this comparison, both methods are demonstrating comparable performance, as each method correctly identified 6 strictly correct candidates. However, the candidate replacements suggested by each method were not always the same, indicating that each method has its own strengths and weaknesses. The advantages of the perplexity-based method is that it is context-aware and better able to handle ambiguous cases. The perplexity-based method can often suggest a more appropriate replacement for a mistyped word when the correct one is not immediately clear, by considering the context of the sentence. This was evident in sentence 15, where the method correctly identified \"iambic\" as the most suitable candidate. Conversely, the frequency-based method is less influenced by noisy training data, as it does not consider the context of the sentence. This can lead to more accurate predictions when the misspelled word is a common misspelling of the correct word. This was beneficial in sentence 10, where the correctly spelled word \"accommodate\" was identified, highlighting the method's capability to avoid being affected by inaccuracies or inconsistencies in the training data. Despite their individual benefits, neither method is perfect, and there is room for improvement in selecting better candidate replacements. \n",
    "\n",
    "\n",
    "**What extra resources/information could we use to pick better candidate replacements?**\n",
    "\n",
    "The following are some suggestions to enhance the correction process:\n",
    "\n",
    "- *Use a combination of methods*: We could combine perplexity and frequency scores, or integrate other scoring methods, to create a more comprehensive ranking system. This can help leverage the strengths of each method and potentially reduce the impact of their individual weaknesses.\n",
    "\n",
    "- *Leverage external resources*: We could employ external resources, such as dictionaries, thesauri, or domain-specific word lists, to provide additional context or refine the candidate selection process. We could also incorporate other sources of information such as syntactic and semantic constraints, as well as using a larger and more diverse training dataset.\n",
    "\n",
    "- *Use ensemble methods*: We could employ multiple language models or algorithms to generate candidate replacements, and then select the best option based on a consensus or weighted vote among them.\n",
    "\n",
    "- *Take into account keyboard layout*: we could consider key proximity when identifying candidate replacements for mistyped words. This approach is based on the idea that typos often occur due to accidentally pressing a nearby key instead of the intended one. To implement this strategy, we can utilize a keyboard distance metric, which calculates the physical distance between keys on a standard keyboard layout. We could compare the mistyped word with possible candidate words to determine which candidates have the smallest cumulative keyboard distance for all the differing characters. By considering keyboard distance as an additional factor, the correction process can be more effective in handling typos that arise from accidental key presses. An example of this is seen in sentence 3 with the mistyped word \"grozn\", where the correct word \"groan\" only differs by a single letter, and these two letters \"z\" and \"a\" are located adjacent to each other on the keyboard layout.\n",
    "\n",
    "It is important to note that perplexity is an interplay between the trained language model and the test set. The performance of the perplexity-based method is contingent on the quality of the trained language model and the representativeness of the test set. If the test set differs significantly from the training data, the perplexity-based method may struggle to provide accurate candidate replacements, as the language model may not have encountered similar contexts during training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
